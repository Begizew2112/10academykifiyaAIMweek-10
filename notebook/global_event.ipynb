{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping of events that affect the price of oil around the world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Your API key\n",
    "api_key = \"0bcf5a2a92944aecaa15477d9275bcac\"\n",
    "\n",
    "# Function to fetch oil-related news articles with pagination\n",
    "def fetch_oil_related_news_with_pagination(api_key, max_pages=5):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    all_articles = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        params = {\n",
    "            'q': \"oil prices OR crude oil OR OPEC OR oil supply OR oil demand\",\n",
    "            'from': \"2024-01-01\",\n",
    "            'sortBy': \"relevancy\",\n",
    "            'language': \"en\",\n",
    "            'pageSize': 100,  # Maximum allowed per page\n",
    "            'page': page,\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "\n",
    "            # Break if no more articles are found\n",
    "            if not articles:\n",
    "                break\n",
    "\n",
    "            all_articles.extend(articles)\n",
    "            print(f\"Page {page}: Retrieved {len(articles)} articles.\")\n",
    "\n",
    "            # Increment page to fetch the next set\n",
    "            page += 1\n",
    "\n",
    "            # Pause to avoid hitting the rate limit\n",
    "            time.sleep(1)  # Adjust sleep time based on your plan's rate limit\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total articles fetched: {len(all_articles)}\")\n",
    "    return all_articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 426 - {\"status\":\"error\",\"code\":\"parameterInvalid\",\"message\":\"You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-10-04, but you have requested 2024-01-01. You may need to upgrade to a paid plan.\"}\n",
      "Total articles fetched: 0\n"
     ]
    }
   ],
   "source": [
    "# Function to save articles to CSV file\n",
    "def save_articles_to_csv(articles, filename=\"oil_news_paginated.csv\"):\n",
    "    headers = [\"title\", \"source\", \"publishedAt\", \"description\", \"url\"]\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for article in articles:\n",
    "            writer.writerow({\n",
    "                \"title\": article['title'],\n",
    "                \"source\": article['source']['name'],\n",
    "                \"publishedAt\": article['publishedAt'],\n",
    "                \"description\": article['description'],\n",
    "                \"url\": article['url']\n",
    "            })\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Fetch articles with pagination\n",
    "articles = fetch_oil_related_news_with_pagination(api_key, max_pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save to CSV if articles are found\n",
    "if articles:\n",
    "    save_articles_to_csv(articles)\n",
    "else:\n",
    "    print(\"No articles found or there was an error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 426 - {\"status\":\"error\",\"code\":\"parameterInvalid\",\"message\":\"You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-10-04, but you have requested 2024-10-04. You may need to upgrade to a paid plan.\"}\n",
      "Total articles fetched: 0\n",
      "No articles found or there was an error.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Your API key\n",
    "api_key = \"0bcf5a2a92944aecaa15477d9275bcac\"\n",
    "\n",
    "# Function to fetch oil-related news articles with pagination\n",
    "def fetch_oil_related_news_with_pagination(api_key, max_pages=5):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    all_articles = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        params = {\n",
    "            'q': \"oil prices OR crude oil OR OPEC OR oil supply OR oil demand\",\n",
    "            'from': \"2024-10-04\",  # Updated date to comply with plan limits\n",
    "            'sortBy': \"relevancy\",\n",
    "            'language': \"en\",\n",
    "            'pageSize': 100,  # Maximum allowed per page\n",
    "            'page': page,\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "\n",
    "            # Break if no more articles are found\n",
    "            if not articles:\n",
    "                break\n",
    "\n",
    "            all_articles.extend(articles)\n",
    "            print(f\"Page {page}: Retrieved {len(articles)} articles.\")\n",
    "\n",
    "            # Increment page to fetch the next set\n",
    "            page += 1\n",
    "\n",
    "            # Pause to avoid hitting the rate limit\n",
    "            time.sleep(1)  # Adjust sleep time based on your plan's rate limit\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total articles fetched: {len(all_articles)}\")\n",
    "    return all_articles \n",
    "# Function to save articles to CSV file\n",
    "def save_articles_to_csv(articles, filename=\"oil_news_paginated.csv\"):\n",
    "    headers = [\"title\", \"source\", \"publishedAt\", \"description\", \"url\"]\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for article in articles:\n",
    "            writer.writerow({\n",
    "                \"title\": article['title'],\n",
    "                \"source\": article['source']['name'],\n",
    "                \"publishedAt\": article['publishedAt'],\n",
    "                \"description\": article['description'],\n",
    "                \"url\": article['url']\n",
    "            })\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Fetch articles with pagination\n",
    "articles = fetch_oil_related_news_with_pagination(api_key, max_pages=5)\n",
    "\n",
    "# Optionally save to CSV if articles are found\n",
    "if articles:\n",
    "    save_articles_to_csv(articles)\n",
    "else:\n",
    "    print(\"No articles found or there was an error.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: Retrieved 100 articles.\n",
      "Error: 426 - {'status': 'error', 'code': 'maximumResultsReached', 'message': 'You have requested too many results. Developer accounts are limited to a max of 100 results. You are trying to request results 100 to 200. Please upgrade to a paid plan if you need more results.'}\n",
      "Total articles fetched: 100\n",
      "Data saved to oil_news_paginated.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Your API key\n",
    "api_key = \"0bcf5a2a92944aecaa15477d9275bcac\"\n",
    "\n",
    "# Function to fetch oil-related news articles with pagination\n",
    "def fetch_oil_related_news_with_pagination(api_key, max_pages=5):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    all_articles = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        params = {\n",
    "            'q': \"oil prices OR crude oil OR OPEC OR oil supply OR oil demand\",\n",
    "            # Temporarily removing 'from' to troubleshoot\n",
    "            # 'from': \"2024-10-30\",  # Comment this out initially if it still fails\n",
    "            'sortBy': \"relevancy\",\n",
    "            'language': \"en\",\n",
    "            'pageSize': 100,  # Maximum allowed per page\n",
    "            'page': page,\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "\n",
    "            # Break if no more articles are found\n",
    "            if not articles:\n",
    "                break\n",
    "\n",
    "            all_articles.extend(articles)\n",
    "            print(f\"Page {page}: Retrieved {len(articles)} articles.\")\n",
    "\n",
    "            # Increment page to fetch the next set\n",
    "            page += 1\n",
    "\n",
    "            # Pause to avoid hitting the rate limit\n",
    "            time.sleep(1)  # Adjust sleep time based on your plan's rate limit\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.json()}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total articles fetched: {len(all_articles)}\")\n",
    "    return all_articles\n",
    "\n",
    "# Function to save articles to CSV file\n",
    "def save_articles_to_csv(articles, filename=\"oil_news_paginated.csv\"):\n",
    "    headers = [\"title\", \"source\", \"publishedAt\", \"description\", \"url\"]\n",
    "    \n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for article in articles:\n",
    "            writer.writerow({\n",
    "                \"title\": article['title'],\n",
    "                \"source\": article['source']['name'],\n",
    "                \"publishedAt\": article['publishedAt'],\n",
    "                \"description\": article['description'],\n",
    "                \"url\": article['url']\n",
    "            })\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Fetch articles with pagination\n",
    "articles = fetch_oil_related_news_with_pagination(api_key, max_pages=5)\n",
    "\n",
    "# Optionally save to CSV if articles are found\n",
    "if articles:\n",
    "    save_articles_to_csv(articles)\n",
    "else:\n",
    "    print(\"No articles found or there was an error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Convert the raw article data into a DataFrame for easier cleaning\n",
    "def clean_articles(articles):\n",
    "    # Load data into a DataFrame\n",
    "    df11 = pd.DataFrame(articles)\n",
    "\n",
    "    # Clean 'title' by removing special characters and extra whitespace\n",
    "    df11['title'] = df11['title'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()) if isinstance(x, str) else x)\n",
    "\n",
    "    # Keep only the source name\n",
    "    df11['source'] = df11['source'].apply(lambda x: x['name'] if isinstance(x, dict) else x)\n",
    "\n",
    "    # Convert 'publishedAt' to a standard date format (e.g., YYYY-MM-DD)\n",
    "    df11['publishedAt'] = pd.to_datetime(df11['publishedAt']).dt.date\n",
    "\n",
    "    # Clean 'description' by removing special characters and extra whitespace\n",
    "    df11['description'] = df11['description'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip()) if isinstance(x, str) else x)\n",
    "\n",
    "    # Drop any rows with missing essential information\n",
    "    df11.dropna(subset=['title', 'source', 'publishedAt', 'description', 'url'], inplace=True)\n",
    "\n",
    "    # Remove duplicates if there are any\n",
    "    df11.drop_duplicates(subset=['title', 'publishedAt'], inplace=True)\n",
    "\n",
    "    print(f\"Cleaned data contains {len(df11)} articles.\")\n",
    "    return df11\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data contains 100 articles.\n",
      "Cleaned data saved to cleaned_oil_news.csv.\n"
     ]
    }
   ],
   "source": [
    "# Clean the articles and save to CSV\n",
    "cleaned_df = clean_articles(articles)\n",
    "cleaned_df.to_csv(r\"C:\\Users\\Yibabe\\Desktop\\10academykifiyaAIMweek-10\\data\\cleaned_oil_news.csv\", index=False, encoding='utf-8')\n",
    "print(\"Cleaned data saved to cleaned_oil_news.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd .read_csv(r'C:\\Users\\Yibabe\\Desktop\\10academykifiyaAIMweek-10\\data\\cleaned_oil_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>fdemott@insider.com (Filip De Mott)</td>\n",
       "      <td>Stock market today: Tech rally leads stocks hi...</td>\n",
       "      <td>The market rebounded from Monday's losing sess...</td>\n",
       "      <td>https://markets.businessinsider.com/news/stock...</td>\n",
       "      <td>https://i.insider.com/67058c9f3f2165d716df5b64...</td>\n",
       "      <td>2024-10-08</td>\n",
       "      <td>US stocks rebounded on Tuesday after Monday's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Insider</td>\n",
       "      <td>Huileng Tan</td>\n",
       "      <td>2 of the world's biggest oil producers are loo...</td>\n",
       "      <td>The US has witnessed a massive rise in oil pro...</td>\n",
       "      <td>https://www.businessinsider.com/major-oil-prod...</td>\n",
       "      <td>https://i.insider.com/6716f1f201ea6d83dee308ac...</td>\n",
       "      <td>2024-10-22</td>\n",
       "      <td>Russia and Saudi Arabia are rethinking their e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                               author  \\\n",
       "0  Business Insider  fdemott@insider.com (Filip De Mott)   \n",
       "1  Business Insider                          Huileng Tan   \n",
       "\n",
       "                                               title  \\\n",
       "0  Stock market today: Tech rally leads stocks hi...   \n",
       "1  2 of the world's biggest oil producers are loo...   \n",
       "\n",
       "                                         description  \\\n",
       "0  The market rebounded from Monday's losing sess...   \n",
       "1  The US has witnessed a massive rise in oil pro...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://markets.businessinsider.com/news/stock...   \n",
       "1  https://www.businessinsider.com/major-oil-prod...   \n",
       "\n",
       "                                          urlToImage publishedAt  \\\n",
       "0  https://i.insider.com/67058c9f3f2165d716df5b64...  2024-10-08   \n",
       "1  https://i.insider.com/6716f1f201ea6d83dee308ac...  2024-10-22   \n",
       "\n",
       "                                             content  \n",
       "0  US stocks rebounded on Tuesday after Monday's ...  \n",
       "1  Russia and Saudi Arabia are rethinking their e...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2024-10-08\n",
       "1    2024-10-22\n",
       "2    2024-10-05\n",
       "3    2024-11-01\n",
       "4    2024-10-20\n",
       "Name: publishedAt, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['publishedAt'] .head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 426 - {'status': 'error', 'code': 'parameterInvalid', 'message': 'You are trying to request results too far in the past. Your plan permits you to request articles as far back as 2024-10-04, but you have requested 1988-01-01. You may need to upgrade to a paid plan.'}\n",
      "Total articles fetched: 0\n",
      "No articles found or there was an error.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Your API key\n",
    "api_key = \"0bcf5a2a92944aecaa15477d9275bcac\"\n",
    "\n",
    "# Function to fetch oil-related news articles with pagination\n",
    "def fetch_historical_oil_news(api_key, start_date, end_date, max_pages=5):\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    all_articles = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        params = {\n",
    "            'q': \"oil prices OR crude oil OR OPEC OR oil supply OR oil demand\",\n",
    "            'from': start_date,\n",
    "            'to': end_date,\n",
    "            'sortBy': \"relevancy\",\n",
    "            'language': \"en\",\n",
    "            'pageSize': 100,  # Maximum allowed per page\n",
    "            'page': page,\n",
    "            'apiKey': api_key\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            articles = data.get('articles', [])\n",
    "\n",
    "            # Break if no more articles are found\n",
    "            if not articles:\n",
    "                break\n",
    "\n",
    "            all_articles.extend(articles)\n",
    "            print(f\"Page {page}: Retrieved {len(articles)} articles.\")\n",
    "\n",
    "            # Increment page to fetch the next set\n",
    "            page += 1\n",
    "\n",
    "            # Pause to avoid hitting the rate limit\n",
    "            time.sleep(1)  # Adjust sleep time based on your plan's rate limit\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.json()}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Total articles fetched: {len(all_articles)}\")\n",
    "    return all_articles\n",
    "\n",
    "# Fetch historical articles between 1988 and 2022\n",
    "start_date = \"1988-01-01\"\n",
    "end_date = \"2022-12-31\"\n",
    "\n",
    "articles = fetch_historical_oil_news(api_key, start_date, end_date, max_pages=5)\n",
    "\n",
    "# Optionally save to CSV if articles are found\n",
    "if articles:\n",
    "    save_articles_to_csv(articles)\n",
    "else:\n",
    "    print(\"No articles found or there was an error.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_oil_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for item in soup.find_all('article'):  # Adjust this based on the actual HTML structure\n",
    "        title = item.find('h2').text if item.find('h2') else 'No title'\n",
    "        link = item.find('a')['href'] if item.find('a') else 'No link'\n",
    "        description = item.find('p').text if item.find('p') else 'No description'\n",
    "        publishedAt = item.find('time')['datetime'] if item.find('time') else 'No date'\n",
    "\n",
    "        articles.append({\n",
    "            'title': title,\n",
    "            'url': link,\n",
    "            'description': description,\n",
    "            'publishedAt': publishedAt\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Example usage\n",
    "url = \"https://oilprice.com/Latest-Energy-News/World-News/\"\n",
    "articles = scrape_oil_news(url)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0 articles and saved to historical_oil_news.csv.\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame and save to CSV\n",
    "df = pd.DataFrame(articles)\n",
    "df.to_csv('historical_oil_news.csv', index=False)\n",
    "print(f\"Scraped {len(df)} articles and saved to historical_oil_news.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'oil_price_impact_gulf_war.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data\n",
    "data = {\n",
    "    \"Event\": [\"Gulf War\"],\n",
    "    \"Price Increase\": [\"Increased from $17 to $36\"],\n",
    "    \"Duration\": [\"9 months\"],\n",
    "    \"Contributing Factors\": [\"Invasion of Kuwait by Iraq\"],\n",
    "    \"Economic Impact\": [\"Contributed to recession in the US\"]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('oil_price_impact_gulf_war.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'oil_price_impact_gulf_war.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gulf = pd.read_csv(r'C:\\Users\\Yibabe\\Desktop\\10academykifiyaAIMweek-10\\notebook\\oil_price_impact_gulf_war.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event</th>\n",
       "      <th>Price Increase</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Contributing Factors</th>\n",
       "      <th>Economic Impact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gulf War</td>\n",
       "      <td>Increased from $17 to $36</td>\n",
       "      <td>9 months</td>\n",
       "      <td>Invasion of Kuwait by Iraq</td>\n",
       "      <td>Contributed to recession in the US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Event             Price Increase  Duration        Contributing Factors  \\\n",
       "0  Gulf War  Increased from $17 to $36  9 months  Invasion of Kuwait by Iraq   \n",
       "\n",
       "                      Economic Impact  \n",
       "0  Contributed to recession in the US  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gulf.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to scrape oil-related news\n",
    "def scrape_oil_news():\n",
    "    url = \"https://www.reuters.com/markets/commodities/\"  # Example URL\n",
    "    response = requests.get(url) \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for item in soup.find_all('article'):\n",
    "        title = item.find('h3').get_text() if item.find('h3') else 'No title'\n",
    "        link = item.find('a')['href'] if item.find('a') else 'No link'\n",
    "        description = item.find('p').get_text() if item.find('p') else 'No description'\n",
    "        published_date = datetime.now().strftime(\"%Y-%m-%d\")  \n",
    "        articles.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'description': description,\n",
    "            'publishedAt': published_date\n",
    "        })\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape and convert to DataFrame\n",
    "oil_news = scrape_oil_news()\n",
    "oil_news_df = pd.DataFrame(oil_news) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped oil news articles saved to 'oil_news.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "oil_news_df.to_csv('oil_news.csv', index=False)\n",
    "print(\"Scraped oil news articles saved to 'oil_news.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No table found on the page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data\n",
    "def scrape_world_bank():\n",
    "    # URL of the page to scrape (replace with actual data page URL)\n",
    "    url = 'https://data.worldbank.org/indicator/NY.GDP.MKTP.CD'  # Example URL for GDP data\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Check if the table exists\n",
    "        table = soup.find('table')\n",
    "        \n",
    "        if table is not None:\n",
    "            # Extract rows from the table\n",
    "            rows = table.find_all('tr')\n",
    "            data = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                cols = [ele.text.strip() for ele in cols]\n",
    "                data.append(cols)\n",
    "            \n",
    "            # Convert to DataFrame for easier manipulation\n",
    "            df = pd.DataFrame(data[1:], columns=data[0])  # Skip the header row\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No table found on the page.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error fetching data:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# Execute the scrape function\n",
    "gdp_data = scrape_world_bank()\n",
    "if gdp_data is not None:\n",
    "    print(gdp_data.head())  # Display the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in the specified elements.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data\n",
    "def scrape_world_bank_gdp():\n",
    "    url = 'https://data.worldbank.org/indicator/NY.GDP.MKTP.CD'  # Example URL for GDP data\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the relevant data elements\n",
    "        # Example: Find all <div> elements with a specific class that contains the data\n",
    "        data_divs = soup.find_all('div', class_='indicator-value')  # Adjust class name based on actual HTML structure\n",
    "        \n",
    "        # Extract text from each div and store in a list\n",
    "        data = [div.text.strip() for div in data_divs if div.text.strip()]\n",
    "\n",
    "        if data:\n",
    "            # Create a DataFrame\n",
    "            df = pd.DataFrame(data, columns=['GDP Value'])  # Adjust column name based on what you're scraping\n",
    "            return df\n",
    "        else:\n",
    "            print(\"No data found in the specified elements.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"Error fetching data:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "# Execute the scrape function\n",
    "gdp_data = scrape_world_bank_gdp()\n",
    "if gdp_data is not None:\n",
    "    print(gdp_data.head())  # Display the first few rows of the DataFrame\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
